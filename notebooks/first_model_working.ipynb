{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae0d34e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.111841Z",
     "start_time": "2023-03-07T15:34:02.411742Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 15:34:02.504738: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 15:34:03.498205: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:03.498255: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-07 15:34:05.093821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:05.094090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:05.094121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow import zeros_like, ones_like, function, GradientTape\n",
    "from tensorflow.train import Checkpoint\n",
    "from tensorflow.random import normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython import display\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf3f72",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9649ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.235907Z",
     "start_time": "2023-03-07T15:34:07.116915Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 15:34:07.182934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-07 15:34:07.183266: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183453: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183562: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183610: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183839: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:34:07.183929: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-07 15:34:07.187362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#BUFFER_SIZE = 60000 #seed for shuffling?\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbf657",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17d99a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.248356Z",
     "start_time": "2023-03-07T15:34:07.238002Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model() :\n",
    "\n",
    "    #Establish the sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Start from latent space (REMEMBER - GENERATOR ARCHITECTURE MUST MIRROR DISCRIMINITOR ARCHITECTURE)\n",
    "    #Noise comes in as (100,0) vector\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "\n",
    "    #Transformer - Transforms output so that mean output -> 0 and SD output -> 1\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    #Activator - weights for generating an image?\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #Reshape - first step towards final image size (separate out neurons)\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "\n",
    "    #Check that output is correct shape\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    #Deconvolute from 7x7x256 to 7x7x128\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "\n",
    "    #Check that output is correct shape\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "\n",
    "    #Transformer - Transforms output so that mean output -> 0 and SD output -> 1\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    #Activator - weights for generating an image?\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #Deconvolute from 7x7x128 to 14x14x64\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "\n",
    "    #Check that output is correct shape\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "\n",
    "    #Transformer - Transforms output so that mean output -> 0 and SD output -> 1\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    #Activator - weights for generating an image?\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #Deconvolute from 14x14x64 to 28x28x64\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    #Transformer - Transforms output so that mean output -> 0 and SD output -> 1\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    #Activator - weights for generating an image?\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #Deconvolute from 28x28x64 to 56x56x3\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    assert model.output_shape == (None, 56, 56, 3)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fc94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75c299a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.436583Z",
     "start_time": "2023-03-07T15:34:07.253826Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c85d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.501553Z",
     "start_time": "2023-03-07T15:34:07.439430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12544)             1254400   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12544)            50176     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 7, 7, 128)        819200    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 64)       204800    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 64)       102400    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 56, 56, 3)        4800      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,436,800\n",
      "Trainable params: 2,411,200\n",
      "Non-trainable params: 25,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fd3a5",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf816347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.509620Z",
     "start_time": "2023-03-07T15:34:07.503943Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(64, (5,5), strides=(2, 2), padding='same', input_shape=[56, 56 , 3]),\n",
    "#         layers.MaxPool2D(pool_size = (2,2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        \n",
    "        layers.Conv2D(128, (3,3), strides = (2,2), padding = 'same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation = 'sigmoid' )])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8c33dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.576636Z",
     "start_time": "2023-03-07T15:34:07.511849Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0196399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.622232Z",
     "start_time": "2023-03-07T15:34:07.581095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        4864      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25089     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,809\n",
      "Trainable params: 103,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbb678",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2baa4c2",
   "metadata": {},
   "source": [
    "### Dicsriminator loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138f2674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.630749Z",
     "start_time": "2023-03-07T15:34:07.625713Z"
    }
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c502a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.639681Z",
     "start_time": "2023-03-07T15:34:07.635708Z"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e0095",
   "metadata": {},
   "source": [
    "### Generator loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11c9b9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.647705Z",
     "start_time": "2023-03-07T15:34:07.642708Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb382288",
   "metadata": {},
   "source": [
    "### Optimizers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b09ae9b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:34:07.661000Z",
     "start_time": "2023-03-07T15:34:07.650697Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(learning_rate = 0.1)\n",
    "discriminator_optimizer = Adam(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906765cb",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1bdb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:33.203535Z",
     "start_time": "2023-03-07T15:14:33.197760Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fece6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de9072",
   "metadata": {},
   "source": [
    "### Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67151c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:35.052039Z",
     "start_time": "2023-03-07T15:14:35.042833Z"
    }
   },
   "outputs": [],
   "source": [
    "@function\n",
    "def train_step(images):\n",
    "    noise = normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with GradientTape() as gen_tape, GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a98f4",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec7791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:36.376568Z",
     "start_time": "2023-03-07T15:14:36.370186Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, :] * 255)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig(f'image_at_epoch_{epoch}.png',)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347917f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:37.784562Z",
     "start_time": "2023-03-07T15:14:37.776956Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Save the model every 10 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "            display.clear_output(wait=True)\n",
    "            generate_and_save_images(generator,epoch + 1, seed)\n",
    "            \n",
    "      # Generate after the final epoch\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2ec07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:37.900763Z",
     "start_time": "2023-03-07T15:14:37.893691Z"
    }
   },
   "outputs": [],
   "source": [
    "test_folder = '/home/paulylydia/code/LimesAndCrimes/project_liminal/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08daf70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:38.247443Z",
     "start_time": "2023-03-07T15:14:38.237802Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import utils\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b36e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:38.757057Z",
     "start_time": "2023-03-07T15:14:38.702757Z"
    }
   },
   "outputs": [],
   "source": [
    "directory = '/home/paulylydia/code/LimesAndCrimes/project_liminal/data'\n",
    "\n",
    "ds = utils.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=5,\n",
    "    image_size=(56, 56),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b69a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:14:40.363586Z",
     "start_time": "2023-03-07T15:14:40.348817Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa647315",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:28:02.788971Z",
     "start_time": "2023-03-07T15:27:48.684930Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "test_output1 = train(ds, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3653a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:27:28.191067Z",
     "start_time": "2023-03-07T15:27:28.185005Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca285e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:21:26.919622Z",
     "start_time": "2023-03-07T15:21:26.660692Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = generator(seed, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d2156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T15:21:54.532607Z",
     "start_time": "2023-03-07T15:21:54.353245Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(predictions[0, :, :, :] * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f1a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
